#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Import modules
import argparse
import os, os.path
import sys
import os
import pandas as pd
import geopandas as gpd
import glob

from shapely import wkt

# %%

# Paths
def mkdir_p(dir):    
    if not os.path.exists(dir):
        os.mkdir(dir)

path_local = '/p/tmp/sazimmer/'
path_local_data = '/p/tmp/mester/'
path_run = path_local + 'FLODIS_NatCat/'
path_run_creator = mkdir_p(path_run)
path_data_name = 'FLODIS_NatCat'
path_results = f"{path_run}results/"
path_results_creator = mkdir_p(path_results)
path_results_merged = f"{path_run}results_merged/"
path_results_merged_creator = mkdir_p(path_results_merged)
max_day_difference = 30

# %%

# Define parser and bath

parser = argparse.ArgumentParser(
    description='schedules run for trend estimation')
parser.add_argument(
    '--shared', action="store_true",
    help='share nodes on cluster')
parser.add_argument(
    '--notify', action="store_true",
    help='notify per mail when done')
parser.add_argument(
    '--minutes', type=int, default=15,
    help='maximal minutes to run on cluster (< 60)')
parser.add_argument(
    '--hours', type=int, default=0,
    help='maximal hours to run on cluster (168=week, 720=month)')
parser.add_argument(
    '--threads', type=int, default=16,
    help='maximal number of threads on cluster (<= 16)')
parser.add_argument(
    '--mem_per_cpu', type=int, default=2000,
    help='number of memory per CPU (3584 is MaxMemPerCPU on cluster)')
parser.add_argument(
    '--largemem', action="store_true",
    help='use ram_gpu partition')
parser.add_argument(
    '--verbose', action="store_true",
    help='be verbose')

args = parser.parse_args()
sys.dont_write_bytecode = True

def schedule_run(split_nr):

    if (int(args.hours) <= 24):
        _class = "short"
    elif (int(args.hours) <= 24 * 7):
        _class = "medium"
    else:
        _class = "long"

    run_params = {
        "job_name": job_name_own,
        "minutes": args.minutes,
        "hours": args.hours,
        "class": _class,  
        "node_usage": "share" if args.shared else "exclusive",
        "notification": "END,FAIL,TIME_LIMIT" if args.notify else "FAIL,TIME_LIMIT",
        "comment": "%s" % (os.getcwd()),
        "environment": "ALL",
        "executable": ("python "+path_data_name+".py '{0}' '{1}' '{2}' '{3}'\n".format(path_local,path_run,split_nr,path_local_data)),
        "num_threads": args.threads,
        "mem_per_cpu": args.mem_per_cpu if not args.largemem else 15360,   # if mem_per_cpu is larger than MaxMemPerCPU then num_threads is reduced
        "other": "#SBATCH --partition=ram_gpu" if args.largemem else ""
    }

    cmd = """echo "#!/bin/sh
#SBATCH --job-name=\\\"%(job_name)s\\\"
#SBATCH --comment=\\\"%(comment)s\\\"
#SBATCH --time=%(hours)02d:%(minutes)02d:00
#SBATCH --qos=%(class)s
#SBATCH --output=/home/sazimmer/output.txt
#SBATCH --error=/home/sazimmer/error.txt
#SBATCH --export=%(environment)s
##SBATCH --mail-type=%(notification)s
#SBATCH --%(node_usage)s
#SBATCH --account=pody 
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=%(num_threads)1d
#SBATCH --mem-per-cpu=%(mem_per_cpu)1d
#SBATCH --workdir=/home/sazimmer/flood_vulnerability_sat/
%(other)s
export OMP_PROC_BIND=true
export OMP_NUM_THREADS=%(num_threads)1d
source activate cama
ulimit -c unlimited
%(executable)s 
" | sbatch -Q""" % run_params

    if args.verbose:
        print(cmd)

    os.system(cmd)

#%%

def schedule_postprocessing():

    if (int(args.hours) <= 24):
        _class = "short"
    elif (int(args.hours) <= 24 * 7):
        _class = "medium"
    else:
        _class = "long"

    run_params = {
        "job_name": "FLODIS_postprocessing",
        "minutes": args.minutes,
        "hours": args.hours,
        "class": _class,  
        "node_usage": "share" if args.shared else "exclusive",
        "notification": "END,FAIL,TIME_LIMIT" if args.notify else "FAIL,TIME_LIMIT",
        "comment": "%s" % (os.getcwd()),
        "environment": "ALL",
        "executable": (path_run+"FLODIS_EMDAT_postprocessing.py '{0}' '{1}'  \n".format(path_local,path_run)),
        "num_threads": args.threads,
        "mem_per_cpu": args.mem_per_cpu if not args.largemem else 15360,   # if mem_per_cpu is larger than MaxMemPerCPU then num_threads is reduced
        "other": "#SBATCH --partition=ram_gpu" if args.largemem else ""
    }

    cmd = """echo "#!/bin/sh
#SBATCH --job-name=\\\"%(job_name)s\\\"
#SBATCH --comment=\\\"%(comment)s\\\"
#SBATCH --time=%(hours)02d:%(minutes)02d:00
#SBATCH --qos=%(class)s
#SBATCH --output=/home/sazimmer/output.txt
#SBATCH --error=/home/sazimmer/errors.txt
#SBATCH --export=%(environment)s
##SBATCH --mail-type=%(notification)s
#SBATCH --%(node_usage)s
#SBATCH --account=pody 
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=%(num_threads)1d
#SBATCH --mem-per-cpu=%(mem_per_cpu)1d
#SBATCH --workdir=/home/sazimmer/flood_vulnerability_sat/
%(other)s
export OMP_PROC_BIND=true
export OMP_NUM_THREADS=%(num_threads)1d
source activate cama
ulimit -c unlimited
%(executable)s 
" | sbatch -Q""" % run_params

    if args.verbose:
        print(cmd)

    os.system(cmd)

#%%

already_done_list = []

for filename in sorted(os.listdir(f"{path_run}results/")):
   # print(filename)
    
 
        
    filename_cut = filename.split("NatCat_single_results_")[1]
    filename_cut = filename_cut.split(".csv")[0]
    filename_cut = int(filename_cut)    
    already_done_list.append(filename_cut)                      
                        
#%%

NatCat = pd.read_csv(path_local_data + '/data/disaster/NatCat/natcat_CPI_adjusted_ppp2011.csv')
NatCat = NatCat[NatCat['year']>=2000]
NatCat = NatCat[NatCat['year']<=2018]

for split_nr in range(len(NatCat)):
    
    if split_nr <= 2:
    

        
        # if filename_temp in already_done_list:
        if split_nr not in already_done_list and split_nr not in [1864,3309]:  # remove cases with NatCat point is within DFO+HydroSheds, but flood extent not within national borders
            
            print(split_nr)
        
           
            
            job_name_own = f"NC_{split_nr}"
            
            
            
            
          #  if filename_temp != 'test': #  'GDIS_mod_2002-0848_0.shp':
#            if filename_temp == 'GDIS_mod_2002-0691_0.shp':
          #  print(split_nr)
            
            
            schedule_run(split_nr)
            
            
            #    schedule_run(filename_temp)
            #    print(counter)
           #     counter += 1
    	       
                # job_name_own = filename_temp.split('GDIS_mod_')[1]
                # if filename_temp == 'GDIS_mod_2002-0483_0.shp':
    
                # 
                # schedule_run(filename_temp)

#%%

df = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', f"{path_results}/*csv"))))
df = df.sort_values(['index'],axis=0,ascending=True)
##df = df.sort_values(['index'],axis=0,ascending=True)# df.to_csv(f"{path_results_merged}EMDAT_GFD_connect_results_FL_merged_geometry.csv")
df = df.drop('geometry', axis=1)
df.to_csv(f"{path_results_merged}NatCat_GFD_connect_results_FL_merged.csv")

#from shapely import wkt

#df['geometry'] = df['geometry'].apply(wkt.loads)
#gdf = gpd.GeoDataFrame(df, crs='epsg:4326')


#df = gpd.GeoDataFrame(df,geometry='geometry')
#gdf.to_file(f"{path_results_merged}EMDAT_GFD_connect_results_FL_merged.shp")
#gdf.to_file(f"{path_results_merged}EMDAT_GFD_connect_results_FL_merged.gpkg", driver="GPKG")


#df = df.drop('geometry', axis=1)
#df.to_csv(f"{path_results_merged}EMDAT_GFD_connect_results_FL_merged.csv")



# job_name_own = 'test'

# schedule_run('GDIS_mod_2001-0417_0.shp')


# df = pd.concat(map(pd.read_csv, glob.glob(os.path.join('', f"{path_results}/*csv"))))
# df = df.sort_values(['disasterno'],axis=0,ascending=True)
# df.to_csv(f"{path_run}results_merge.csv") 


